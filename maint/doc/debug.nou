Debug performance of application (mutt)
  +++ http://www.codeblueprint.co.uk/2016/12/19/a-kernel-devs-approach-to-improving.html
      http://www.codeblueprint.co.uk/2017/01/15/a-kernel-devs-approach-to-improving2.html
  It's a good idea to look at the state of the entire system before delving into application-specific issues
  Get system stats when running application
    $ vmstat 1
    # procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----
    # r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st
    # 1  0      0 4761080  2316 1994984    0    0 55392  3268 7571 30468  9 7 72 13  0
  Get per-process results
    $ pidstat -C mutt 1
    # 21:53:07      UID       PID    %usr %system  %guest    %CPU   CPU  Command
    # 21:53:08     1000      4475   31.00   22.00    0.00   53.00     0  mutt
  Sum delays of sched_stat_iowait tracepoint
    # kworker/u8:5-7166  [001] d..4  78.852761: sched_stat_iowait: comm=mutt pid=7200 delay=804910 [ns]
    $ grep -E "iowait.*mutt" trace  | sed -e 's/.*delay=//' | \
      awk 'BEGIN{s=0}{s+=$1} END{printf "%.2fs\n", s/1000000000}'
  Acquire per-CPU distribution of time
    $ perf report -g fractal --no-children
    # +    5.07%  mutt     libc-2.24.so        [.] __strchr_sse2
    # ...


Overhead of context switch
  TUT ~ 5-7uS
    ++ https://stackoverflow.com/questions/21887797/what-is-the-overhead-of-a-context-switch
    https://www.linuxjournal.com/article/2941
    https://serverfault.com/questions/14199/how-many-context-switches-is-normal-as-a-function-of-cpu-cores-or-other
  Per process
    $ grep ctxt /proc/$pid/status
      voluntary_ctxt_switches:        41
      nonvoluntary_ctxt_switches:     16
    $ pidstat -wt 10 1
    $ sudo perf stat -e context-switches -I 1000 PROCESS_NAME
    $ pmap
  System total
    $ sar -w 1 3
      07:29:32 PM    proc/s   cswch/s
      07:29:33 PM     13.00   1343.00
    $ sudo perf record -e context-switches -a -g
      <ctrl+c>
      $ sudo perf report
    $ powertop
    $ lmbench / lat_ctx
      http://www.bitmover.com/lmbench/lat_ctx.8.html
    $ mpstat -P ALL
    $ dstat


INFO
  https://www.slashroot.in/linux-cpu-performance-monitoring-tutorial
  * If the us column in vmstat goes too high in the output, without a similar
    level of context switches, then it is possible that a single process is using
    the processor for a substantially higher amount of time
  * If the  wa column also experiences a too high value at the same time (when
    us is higher and cs is substantially low ), that means the process that
    caused the  us value to stay high is doing some heavy i/o operation.
  * A case where you have higher number of interrupts (shown by in column in
    vmstat) and substantially lower number of context switches (shown by cs) can
    indicate that a particular application (which might be single threaded) is
    sending too many hardware requests.


Interrupts
  https://forums.xilinx.com/t5/Embedded-Linux/Interrupt-numbers-changed-in-recent-kernels/td-p/685685
  +++ REF: https://elinux.org/images/8/8c/Zyngier.pdf
  +++ https://wiki.linaro.org/WorkingGroups/PowerManagement/Doc/WakeUpSources
    * "GIC" = SEE: ARM Generic Interrupt Controller - Architecture Specification
    * "twd" = local timers are referred as twd/GIC=29 which stands for Timer Watchdog
    * "IPI" : Inter Processor Interrupt
    NOTE
      - when the number of interrupts for the global timer is much greater than
        the local timer, that indicates the system is mostly idle
  https://github.com/torvalds/linux/blob/master/Documentation/devicetree/bindings/interrupt-controller/arm%2Cgic.txt
  http://billauer.co.il/blog/2012/08/irq-zynq-dts-cortex-a9/
    https://forums.xilinx.com/t5/Embedded-Linux/Interrupt-numbers-changed-in-recent-kernels/td-p/685685


IOWait
  !! https://blog.pregos.info/wp-content/uploads/2010/09/iowait.txt
    https://serverfault.com/questions/12679/can-anyone-explain-precisely-what-iowait-is
    https://gist.github.com/haridsv/0d65d263b7f5f79a73e5
  [%idle=0 and %iowait=0 and e.g. %cpu=100%]
    <= CPU always has work to do, switching contexts betwee high-cpu-bound processes
    <= you don't know how many processes are waiting on slow I/O
